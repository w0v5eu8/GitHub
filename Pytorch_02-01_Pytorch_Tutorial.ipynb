{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치(pytorch) 기초\n",
    "\n",
    "- 파이토치는 딥 러닝 연구 혹은 상용 제품을 빠르게 개발하도록 해주는 오픈 소스 머신러닝 프레임워크이다.\n",
    "- 파이토치는 사용자 친화적인 라이브러리로써 구현된 다양한 도구를 통해 빠르고 유연한 실험 및 효과적인 상용화를 가능하게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 텐서의 기초\n",
    "### 텐서란 무엇인가?\n",
    "텐서(Tensor)는 넘파이 배열(Numpy array)과 마찬가지로 다차열 배열(Multi dimensional array)을 표현할 수 있다. 따라서 텐서는 넘파이 배열과 동작이 매우 유사한데, 넘파이 배열과는 다르게 GPU 기기에 올려서 계산을 더 빠르게 해줄 수 있다는 장점이 있다. 여기서는 어떻게 텐서를 정의하는지, 넘파이 배열과 어떻게 다른지에 대한 내용을 다룬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치는 torch 로 임포트한다.\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3]) # 넘파이 배열 선언\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([1, 2, 3]) # 파이토치 텐서 선언 (넘파이 배열 선언과 매우 유사하다.)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```torch.from_numpy()```메서드를 이용하여 넘파이 배열은 파이토치 텐서로 쉽게 변환 가능하다. 또한 반대로도 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "a_tensor = torch.from_numpy(a)\n",
    "\n",
    "print(type(a_tensor))\n",
    "a_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([1, 2, 3])\n",
    "b_numpy = b.numpy()\n",
    "\n",
    "print(type(b_numpy))\n",
    "b_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원 텐서 선언 (넘파이 배열에서 쓰는 attribute 및 함수들과 매우 유사)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor([[1, 2, 3], [4, 5, 6]]) # 2차원 텐서 선언\n",
    "print('c: \\n',c.__repr__())\n",
    "print('shape of c: ', c.shape)\n",
    "print('dimension of c: ', c.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러가지 텐서 선언 방법 (넘파이 함수들과 매우 유사)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(0, 10, 1)\n",
    "print('created from .arange() method: ', a)\n",
    "\n",
    "a = torch.zeros(10)\n",
    "print('created from .zeros() method: ', a)\n",
    "\n",
    "a = torch.ones(10)\n",
    "print('created from .ones() method: ', a)\n",
    "\n",
    "a = torch.linspace(0,2,9)\n",
    "print('created from .linspace() method: ', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다차원 텐서 선언\n",
    "a = torch.randn(size=(2, 4, 5)) # uniform 분포를 따르는 랜덤 값 반환\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dimension of tensor: ', a.ndim)\n",
    "print('shape of tensor', a.shape) \n",
    "print('total number of elements in tensor: ', a.numel())  # 넘파이 배열과 다르다!\n",
    "print('data type of elements: ', a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치 텐서 또한 넘파이 배열과 마찬가지로 **인덱싱(Indexing)**과 **슬라이싱(slicing)**을 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[7]  # indexing 7'th element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[3:9]  # slicing from 3 to 9 (exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[3:]  #slicing from 3 to the last element (inclusive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서의 shape 변경 (```torch.Tensor.reshape()``` 또는 ```torch.Tensor.view()``` 메서드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3, 4) shape의 텐서 선언\n",
    "a = torch.arange(12).reshape(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(12).view(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서의 동작 및 연산\n",
    "\n",
    "텐서의 연산 또한 지난 시간에 다룬 넘파이 배열의 연산과 크게 다르지 않다. +, - 와 같은 사칙연산은 ```torch.add()```, ```torch.sub()```과 같은 메서드로도 구현될 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(5)\n",
    "b = torch.arange(4, -1, -1)\n",
    "print('a:\\n', a)\n",
    "print('b:\\n', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support all basic numerical operations such as +. -. *, /, ** ..\n",
    "print('a + b: ', a + b)\n",
    "print('a + b: ', torch.add(a, b))  # torch.add() 메서드를 사용\n",
    "print('a - b: ', a - b)\n",
    "print('a - b: ', torch.sub(a, b))  # torch.sub() 메서드를 사용\n",
    "print('a^2: ', a ** 2)\n",
    "print('cos(a): ', np.cos(a))\n",
    "print('logical operation of a < 1: ', a < 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose 연산 (```torch.Tensor.t()``` 메서드 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(10).reshape(2, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose\n",
    "a.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```torch.cat()``` 메서드를 이용하여 텐서 붙이기 (쌓기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2, 3],[4 ,5, 6]])\n",
    "b = torch.tensor([[7, 8, 9],[10, 11, 12]])\n",
    "print('a:\\n', a)\n",
    "print('b:\\n', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위로(dim=0) 쌓기 (stack vertically)\n",
    "torch.cat([a, b], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옆으로(dim=1) 쌓기 (stack horizontally)\n",
    "torch.cat([a, b], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```torch.Tensor.item()```메서드로 스칼라(rank0) 텐서를 파이썬 넘버로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(10.) # 스칼라 텐서\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.item()  # 파이썬 넘버로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텐서를 GPU에 올리기\n",
    "\n",
    "해당 머신에서 GPU가 사용가능한지 판단하고, 만약 사용이 가능하다면 빠른 계산을 위하여 텐서를 GPU에 올린다.\n",
    "\n",
    "**GPU가 사용가능한지 판단하는 메서드**: ```torch.cuda.is_available()```의 반환 값이 True 이면 사용가능, False 이면 CPU만 사용가능\n",
    "\n",
    "\n",
    "**텐서를 GPU에 올리는 메서드**: ```torch.Tensor.to(torch.device)``` 혹은 ```torch.Tensor.cuda()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용가능한 GPU가 있는지 확인\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3]).cuda('cuda:4')  # specify the GPU number\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([4, 5, 6])\n",
    "\n",
    "device = torch.device('cuda:4')  # CUDA device object\n",
    "y = y.to(device)  # using .to() method\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x + y\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU에 올라가 있는 텐서를 CPU로 되돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 자동미분(Autograd)\n",
    "### ```backward()```를 이용한 미분(gradient) 계산\n",
    "\n",
    "딥러닝 분야에서 어떤 objective function의 미분 값을 계산하고 경사하강(gradient descent) 방식을 도입하여 최적의 파라미터를 찾는다. 이때 미분 값의 계산을 용이하게 해주는 것이 파이토치의 자동미분 기능이다. 파이토치는 행하는 모든 연산을 저장할 수 있고 이를 활용하여 해당 연산에 대한 미분 값을 계산함으로써 다차원 파라미터에 대한 미분도 효율적으로 처리할 수 있다.\n",
    "\n",
    "**미분 계산을 위한 텐서 옵션**\n",
    "\n",
    "미분계산을 위해 연산을 트래킹하기 위해서 텐서의 옵션 중 requires_grad가 True여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 연산을 tracking하기 위해 requires_grad=True로 설정\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 연산을 tracking. .grad_fn 으로 확인가능\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y ** 2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```backward()```** 메서드로 미분 값 계산\n",
    "\n",
    "backward() 함수는 스칼라(scalar) 텐서에 대해서만 사용가능(보통 손실함수 및 objective function은 scalar 이다). 위의 텐서 z의 element들을 모두 더해서 스칼라로 만들어준 후 x에 대해 미분하여 미분 값을 계산해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sum = z.sum()\n",
    "z_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward() 메서드 호출 시 x에 대한 미분값이 자동으로 계산\n",
    "z_sum.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x 에 대한 미분 값이 계산되면 .grad 로 미분 값 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad # x 가 4개의 element로 이루어져 있으므로 gradient도 4개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한번 더 호출하면 에러 발생\n",
    "z_sum.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한번 더 호출하려면 연산그래프 (computational graph)를 유지를 해야하는데, 이는 다음과 같이 설정하면 된다.\n",
    "y = x + 2\n",
    "z = y ** 2\n",
    "z_sum = z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sum.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain_graph=True로 설정하면 한번 더 backward 호출 가능\n",
    "z_sum.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 볼 수 있듯이, ```backward()``` 메서드를 호출할 때 마다 새로 계산된 미분 값으로 치환되는 것이 아닌 기존의 저장된 미분 값에서 누적이되는 형태이다. 따라서 새로운 미분 값만 저장하고 싶다면, ```.grad.data.zero_()``` 메서드를 이용하여 기존의 저장된 미분 값을 제거하고 ```backward()``` 메서드를 호출하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존의 미분 값 제거\n",
    "x.grad.data.zero_()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sum.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires_grad 설정이 안된 상태로 선언된 텐서도 사후설정 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float)\n",
    "x.requires_grad  # required_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True  # True로 설정 가능\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```with```문으로 미분계산 tracking 끊기**\n",
    "\n",
    "모든 파라미터에 대한 미분 값을 계산하기 위해서는 해당 연산(computational graph)을 모두 기억해야하므로 메모리 부담이 크다. 간혹 파라미터에 대한 연산 기억이 필요하지 않을 때가 있는데(예를 들면 validation 이나 test 시), 이 때에 메모리 부담을 줄이기 위해 중간에 연산 tracking을 끊는 방식을 사용하기도 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad() 블록 안에서 행해지는 모든 연산은 메모리에 저장되지 않는다. 따라서 미분 계산도 불가능\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    z = y ** 2\n",
    "\n",
    "z.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 선언 및 학습\n",
    "### 파이썬 클래스의 기초\n",
    "\n",
    "**파이썬 클래스 정의**\n",
    "\n",
    "파이썬 클래스는 키워드 ```class```와 콜론(:)을 이용하여 다음과 같이 정의한다.\n",
    "\n",
    "```python\n",
    "class 클래스이름:\n",
    "    \n",
    "    def 메서드이름(self, ...):\n",
    "        statements\n",
    "        \n",
    "    def ...\n",
    "    \n",
    "    ...\n",
    "```\n",
    "\n",
    "클래스는 여러 메서드와 attribute들을 가지고있는 일종의 이름공간이다. 가장 간단한 클래스를 정의해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S1:\n",
    "    a = 10\n",
    "    \n",
    "S1.a  # 클래스 S1의 attribute인 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1.b = 20  # 클래스 S1에 새로운 attribute인 b를 만들어서 20을 저장\n",
    "S1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**인스턴스 만들기**\n",
    "\n",
    "아래와 같이 정의된 클래스를 호출하면 인스턴스가 만들어진다. 일단 클래스가 하나 정의되면 인스턴스는 여러개 만들 수 있고 각각의 인스턴스는 독립적으로 행동한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst1 = S1()  # inst1 이라는 인스턴스 만들기\n",
    "print(inst1.a)  # 클래스가 가지고 있는 메서드와 attribute를 모두 가지고 있는 새로운 이름공간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inst1.b)  # 위에서 클래스 S1에 만든 새로운 attribute인 b도 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst1.a = 100  # 인스턴스 이름공간의 attribute 값 변경\n",
    "print(inst1.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하지만 새로 만들어진 인스턴스에는 영향이 없다. (독립적으로 기능)\n",
    "inst2 = S1()  #inst2 라는 인스턴스 만들기\n",
    "print(inst2.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**클래스 안에서 메서드 정의하기**\n",
    "\n",
    "클래스 내에 메서드를 정의할 땐 ```self```를 사용하며 이는 메서드 인스턴스 그 자체를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    # 인스턴스 초기화 메서드 -> 인스턴스를 만듦과 동시에 호출되는 함수이다.\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # self 가 인스턴스 그 자체이므로 이 statement는 인스턴스 이름공간 내에 name이라는 attribute를 만든다는 의미.\n",
    "        \n",
    "    def whoami(self):\n",
    "        return 'You are ' + self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person1 = Person(name='당신의 이름')  # 인스턴스를 만들면 자동으로 __init__() 메서드 호출\n",
    "person1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 메서드 호출\n",
    "person1.whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 메서드 호출은 메서드 내에서도 가능하다.\n",
    "# 값을 1씩 count 하는 클래스 정의\n",
    "\n",
    "class Myclass:\n",
    "    \n",
    "    def __init__(self, v):\n",
    "        self.value = v\n",
    "    \n",
    "    def get(self):\n",
    "        return self.value\n",
    "    \n",
    "    def count(self):\n",
    "        self.value = self.value + 1\n",
    "        return self.get()  # 메서드 내에서 클래스 메서드 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Myclass(v=10)\n",
    "counter.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.count()  #  + 1씩 카운트\n",
    "counter.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**클래스 상속**\n",
    "\n",
    "클래스에는 상속이라는 개념이 존재하는데, 클래스를 상속할 시 상속받는 클래스는 상속하는 클래스의 모든 메서드나 attribute를 가지고 있다. 단, 상속받는 클래스에 상속하는 클래스의 메서드와 같은 이름의 메서드가 존재할 때, 상속받는 클래스의 메서드로 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상속하는 클래스 parent 정의\n",
    "class parent:\n",
    "    def __init__(self):\n",
    "        self.parent_attr = 'I am a parent'\n",
    "        \n",
    "    def parent_method(self):\n",
    "        print('call parent method..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent 클래스 상속을 받는 child 클래스 정의\n",
    "class child(parent):\n",
    "    def __init__(self):\n",
    "        parent.__init__(self)  # parent와 같은 메서드 이름인 __init__이 child안에 정의되어 있으므로 \n",
    "                               # parent_attr도 가져오기 위해서는 Parent 클래스에서 __init__ 메서드 호출\n",
    "        self.child_attr = 'I am a child'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = child()\n",
    "c1.parent_method()  # 상속 클래스 메서드 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.parent_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.child_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class child(parent):\n",
    "    def __init__(self):\n",
    "        self.child_attr = 'I am a child'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = child()\n",
    "c1.parent_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class child(parent):\n",
    "    def __init__(self):\n",
    "        self.child_attr = 'I am a child'        \n",
    "        \n",
    "    def parent_method(self):  # 상속 클래스에 있는 메서드와 같은 이름의 메서드를 선언\n",
    "        print(self.child_attr)\n",
    "        \n",
    "c1 = child()\n",
    "c1.parent_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class child(parent):\n",
    "    def __init__(self):\n",
    "        self.child_attr = 'I am a child'        \n",
    "        \n",
    "    def parent_method(self):  # 상속 클래스에 있는 메서드와 같은 이름의 메서드를 선언\n",
    "        parent.parent_method(self)  # 기존의 parent 클래스에 있는 parent_method 메서드도 호출\n",
    "        print(self.child_attr)\n",
    "        \n",
    "c1 = child()\n",
    "c1.parent_method()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```super()``` 함수**\n",
    "\n",
    "```super()```함수는 ```parent.parent_method(self)```와 마찬가지로 상속하는 클래스의 메서드를 호출할 때 사용한다. ```super()```함수가 자동으로 상속 클래스를 잡아준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class child(parent):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # parent_attr도 가져오기 위해서는 Parent 클래스에서 __init__ 메서드 호출\n",
    "        self.child_attr = 'I am a child'        \n",
    "        \n",
    "    def parent_method(self):  # 상속 클래스에 있는 메서드와 같은 이름의 메서드를 선언\n",
    "        super().parent_method()  # 기존의 parent 클래스에 있는 parent_method 메서드도 호출\n",
    "        print(self.child_attr)\n",
    "        \n",
    "c1 = child()\n",
    "c1.parent_method()\n",
    "print(c1.parent_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module 을 상속하여 모델 선언\n",
    "\n",
    "보통 다음과 같은 형태로 nn.Module을 상속하여 클래스 안에서 모델을 설계함\n",
    "\n",
    "```python\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.module1 = ...\n",
    "        self.module2 = ...\n",
    "        ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.module1(x)\n",
    "        x = self.module2(x)\n",
    "        ...\n",
    "```\n",
    "\n",
    "위와 같이 정의하고 다음과 같이 사용\n",
    "\n",
    "```python\n",
    "net = Network()\n",
    "output = net(input)\n",
    "loss = loss_function(output, target)\n",
    "net.zero_grad()\n",
    "loss.backward()\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear classifier 설계\n",
    "class Net(nn.Module):  # nn.Module 상속\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear = nn.Linear(2, 1)  # input 2 dim, output 1 dim 인 linear classifier 정의\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 설계한 모델 인스턴스를 선언\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.Module을 상속하여 설계한 모델을 업데이트 하기위한 클래스 (```torch.optim```)**\n",
    "\n",
    "```net.parameters()``` 로 업데이트를 해야하는 weight을 넘겨주고 learning rate (lr)을 설정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**가우시안 데이터 상황에서의 예시 (1주차)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train data 생성\n",
    "np.random.seed(100)\n",
    "n = 100 # data point 개수\n",
    "X1 = np.random.normal(loc=(5, 10), scale=5, size=(n, 2)) # loc: 평균, scale: 분산\n",
    "X2 = np.random.normal(loc=(20, 20), scale=5, size=(n, 2)) # X1의 데이터와 X2의 데이터들이 서로 다른 평균 좌표를 갖도록 설정\n",
    "Y1 = torch.ones(n) \n",
    "Y2 = torch.ones(n) * -1 # X1의 데이터들에는 label 1을, X2의 데이터들에는 label -1를 부여\n",
    "\n",
    "X_train = torch.tensor(np.concatenate((X1, X2)), dtype=torch.float) # X1과 X2를 concatenate 하여 X_train 생성\n",
    "Y_train = torch.cat((Y1, Y2)) # Y1과 Y2를 concatenate 하여 Y_train 생성\n",
    "\n",
    "# Train data plot\n",
    "plt.scatter(X1[:, 0].T, X1[:, 1].T, color='b', edgecolor='k', label='label : 1', s=35) # s: 점크기\n",
    "plt.scatter(X2[:, 0].T, X2[:, 1].T, color='r', edgecolor='k', label='label : -1', s=35)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predict, y):\n",
    "    \n",
    "    hard_pred = 2 * (predict >= 0).type(torch.float) - 1   # predict 값이 0 이상이면 hard_pred=1, predict 값이 0 미만이면 hard_pred=-1\n",
    "    acc = (hard_pred == y).type(torch.float).mean() * 100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "num_of_iteration = 20000\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(100)\n",
    "\n",
    "\n",
    "it = tqdm(range(num_of_iteration))\n",
    "for i in it:\n",
    "\n",
    "    # 위에서 선언한 모델을 이용하여 output 계산 (이 경우에는 weight과의 내적)\n",
    "    output = net(X_train)\n",
    "    \n",
    "    # output 과 label(target)을 이용한 손실 값 계산\n",
    "    loss = (output * (torch.sign(output) - Y_train.reshape(-1, 1))).mean()\n",
    "\n",
    "    # Train accuracy 계산\n",
    "    train_acc = accuracy(output, Y_train.reshape(-1, 1))      \n",
    "      \n",
    "    # Backward 함수를 이용한 gradient 계산\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient descent를 이용한 모델 update\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        it.set_postfix(accuracy='{:.2f}'.format(train_acc),\n",
    "                      loss='{:.4f}'.format(loss))\n",
    "    \n",
    "# Train accuracy 및 test accuracy 계산\n",
    "predict = torch.sign(net(X_train))\n",
    "train_acc = accuracy(predict, Y_train.reshape(-1, 1))\n",
    "print('train accuracy: {:.2f}'.format(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X1[:, 0].T, X1[:, 1].T, color='b', edgecolor='k', label='label : 1', s=35)\n",
    "plt.scatter(X2[:, 0].T, X2[:, 1].T, color='r', edgecolor='k', label='label : -1', s=35)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "axes = plt.gca() \n",
    "x_min, x_max = axes.get_xlim() \n",
    "y_min, y_max = axes.get_ylim()\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 30), np.linspace(y_min, y_max, 30)) # 30 grids for each axis\n",
    "grids = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float)\n",
    "Z = net(grids).detach()\n",
    "plt.contour(xx, yy, Z.reshape(xx.shape), levels=[0], colors='k')\n",
    "\n",
    "plt.title('Decision Boundary with Train Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU에 모델 올리기**\n",
    "\n",
    "위의 GPU에 텐서를 올리는 방식과 같은 방식으로 모델과 데이터 모두 GPU에 올릴 수 있다.\n",
    "GPU에 올려서 실행하면 더 빠르고 효율적인 학습이 가능하다.\n",
    "\n",
    "위의 코드에서 데이터, 모델에만 ```.cuda()``` 혹은 ```.to(device)``` 함수를 실행하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 사용 가능한지 확인\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "num_of_iteration = 20000\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(100)\n",
    "\n",
    "# GPU에 올려서 학습\n",
    "X_train = X_train.cuda('cuda:4')\n",
    "Y_train = Y_train.cuda('cuda:4')\n",
    "net.cuda('cuda:4')  # in-plcace 방식으로 동작!\n",
    "\n",
    "it = tqdm(range(num_of_iteration))\n",
    "for i in it:\n",
    "\n",
    "    # 위에서 선언한 모델을 이용하여 output 계산 (이 경우에는 weight과의 내적)\n",
    "    output = net(X_train)\n",
    "    \n",
    "    # output 과 label(target)을 이용한 손실 값 계산\n",
    "    loss = (output * (torch.sign(output) - Y_train.reshape(-1, 1))).mean()\n",
    "\n",
    "    # Train accuracy 계산\n",
    "    train_acc = accuracy(output, Y_train.reshape(-1, 1))      \n",
    "      \n",
    "    # Backward 함수를 이용한 gradient 계산\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient descent를 이용한 모델 update\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        it.set_postfix(accuracy='{:.2f}'.format(train_acc),\n",
    "                      loss='{:.4f}'.format(loss))\n",
    "    \n",
    "# Train accuracy 및 test accuracy 계산\n",
    "predict = torch.sign(net(X_train))\n",
    "train_acc = accuracy(predict, Y_train.reshape(-1, 1))\n",
    "print('train accuracy: {:.2f}'.format(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X1[:, 0].T, X1[:, 1].T, color='b', edgecolor='k', label='label : 1', s=35)\n",
    "plt.scatter(X2[:, 0].T, X2[:, 1].T, color='r', edgecolor='k', label='label : -1', s=35)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "axes = plt.gca() \n",
    "x_min, x_max = axes.get_xlim() \n",
    "y_min, y_max = axes.get_ylim()\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 30), np.linspace(y_min, y_max, 30)) # 30 grids for each axis\n",
    "grids = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float)\n",
    "\n",
    "net.cpu()  # grids가 cpu위에 있으므로 모델도 같은 머신위에 올려야 연산이 가능하다.\n",
    "Z = net(grids).detach()\n",
    "plt.contour(xx, yy, Z.reshape(xx.shape), levels=[0], colors='k')\n",
    "\n",
    "plt.title('Decision Boundary with Train Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고문헌\n",
    "\n",
    "[What is PyTorch?] https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py  \n",
    "[AUTOGRAD: AUTOMATIC DIFFERENTIATION] https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py  \n",
    "[A Beginner-Friendly Guide to PyTorch and How it Works from Scratch] https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/  \n",
    "[What is PyTorch and how does it work?] https://hub.packtpub.com/what-is-pytorch-and-how-does-it-work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
